{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72937efdb762938"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mip import Model, xsum, maximize\n",
    "import cornac\n",
    "from cornac.eval_methods import BaseMethod\n",
    "from cornac.models import MF, WMF, VAECF, SVD\n",
    "from cornac.metrics import Precision, Recall, NDCG\n",
    "from cornac.data import Reader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "760ce558cc787365",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "127d499211ef1cc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds_names = [\"Tracks\"]\n",
    "ds_users = ['005']\n",
    "ds_items = ['020']\n",
    "\n",
    "no_user_groups = 2\n",
    "no_item_groups = 2\n",
    "topk = 50  # this is not a length of recommendation ist, it is only the first topk items for the optimisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74eb88c1378ee380",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load `Cornac` data and model\n",
    "### Reading the train, test, and val sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2db3d6acc833d91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_data(root_path):\n",
    "    \"\"\"\n",
    "    Read the train, test, and tune file using Cornac reader class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : the name of the dataset\n",
    "      example: 'MovieLens100K'\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    train_data:\n",
    "      The train set that is 70% of interactions\n",
    "    tune_data:\n",
    "      The tune set that is 10% of interactions\n",
    "    test_data:\n",
    "      The test set that is 20% of interactions\n",
    "    \"\"\"\n",
    "    reader = Reader()\n",
    "    train_data = reader.read(fpath=f'{root_path}/train_ratings.txt', fmt='UIR', sep=' ')\n",
    "    tune_data = reader.read(fpath=f'{root_path}/val_ratings.txt', fmt='UIR', sep=' ')\n",
    "    test_data = reader.read(fpath=f'{root_path}/test_ratings.txt', fmt='UIR', sep=' ')\n",
    "    return train_data, tune_data, test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd53ff5234df5ad8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data into Cornac evaluation method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c67af06500fa9b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data):\n",
    "    \"\"\"\n",
    "    load data into Cornac evaluation method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data:\n",
    "      train_data from Reader Class\n",
    "    test_data:\n",
    "      test_data from Reader Class\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    eval_method:\n",
    "      Instantiation of a Base evaluation method using the provided train and test sets\n",
    "    \"\"\"\n",
    "    # exclude_unknowns (bool, default: False) – Whether to exclude unknown users/items in evaluation.\n",
    "    # Instantiate a Base evaluation method using the provided train and test sets\n",
    "    eval_method = BaseMethod.from_splits(\n",
    "        train_data=train_data, test_data=test_data, rating_threshold=1.0, exclude_unknowns=True, verbose=True\n",
    "    )\n",
    "\n",
    "    return eval_method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efdd3841ec31cd86",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the cornac"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65d99e3b14319cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_model(eval_method):\n",
    "    \"\"\"\n",
    "    running the cornac\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_method:\n",
    "      Cornac's evaluation protocol\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    exp:\n",
    "      Cornac's Experiment\n",
    "    \"\"\"\n",
    "\n",
    "    models = [\n",
    "        MF(\n",
    "            max_iter=5,\n",
    "            k=1000,\n",
    "            early_stop=True,\n",
    "            verbose=True,\n",
    "            lambda_reg=0.001,\n",
    "            learning_rate=0.01,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        WMF(\n",
    "            k=1000,\n",
    "            max_iter=50,\n",
    "            learning_rate=0.001,\n",
    "            lambda_u=0.01,\n",
    "            lambda_v=0.01,\n",
    "            verbose=True,\n",
    "        ),\n",
    "        SVD(\n",
    "            max_iter=5,\n",
    "            k=1000,\n",
    "            early_stop=True,\n",
    "            verbose=True,\n",
    "            lambda_reg=0.0001,\n",
    "            learning_rate=0.0001\n",
    "        ),\n",
    "        VAECF(\n",
    "            k=1000,\n",
    "            autoencoder_structure=[20],\n",
    "            act_fn=\"tanh\",\n",
    "            likelihood=\"mult\",\n",
    "            n_epochs=100,\n",
    "            batch_size=100,\n",
    "            learning_rate=0.001,\n",
    "            beta=1.0,\n",
    "            use_gpu=True,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # define metrics to evaluate the models\n",
    "    metrics = [\n",
    "        Precision(k=5),\n",
    "        Precision(k=10),\n",
    "        Precision(k=20),\n",
    "        Precision(k=50),\n",
    "        Recall(k=5),\n",
    "        Recall(k=10),\n",
    "        Recall(k=20),\n",
    "        Recall(k=50),\n",
    "        NDCG(k=5),\n",
    "        NDCG(k=10),\n",
    "        NDCG(k=20),\n",
    "        NDCG(k=50)\n",
    "    ]\n",
    "\n",
    "    # put it together in an experiment, voilà!\n",
    "    exp = cornac.Experiment(eval_method=eval_method, models=models, metrics=metrics)\n",
    "    exp.run()\n",
    "\n",
    "    return exp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab14f546cb08674d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load user and item groups\n",
    "Create a set of IDs for each users group\n",
    "Create a matrix U which shows the user and the groups of the user"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3591c0562ed4754e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_user_groups(user_group_fpath: str, gid) -> set:\n",
    "    \"\"\"\n",
    "    Read the user groups lists\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_group_fpath:\n",
    "      The path of the user group file\n",
    "\n",
    "    U (global variabvle):\n",
    "      The global matrix of users and their group\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    user_ids:\n",
    "      The set of user ids corresponding to the group\n",
    "    \"\"\"\n",
    "\n",
    "    user_group = open(user_group_fpath, 'r').readlines()\n",
    "    user_ids = set()\n",
    "    for eachline in user_group:\n",
    "        uid = eachline.strip()\n",
    "        # convert uids to uidx\n",
    "        if uid in eval_method.train_set.uid_map:\n",
    "            uid = eval_method.train_set.uid_map[uid]\n",
    "            uid = int(uid)\n",
    "            user_ids.add(uid)\n",
    "            U[uid][gid] = 1\n",
    "    return user_ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f22da98af12aca79",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d73382ca6e77681"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_ground_truth(test_file):\n",
    "    \"\"\"\n",
    "    Read test set data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_file:\n",
    "      The test set data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    ground_truth:\n",
    "      A dictionary includes user with actual items in test data\n",
    "    \"\"\"\n",
    "    ground_truth = defaultdict(set)\n",
    "    truth_data = open(test_file, 'r').readlines()\n",
    "    for eachline in truth_data:\n",
    "        uid, iid, _ = eachline.strip().split()\n",
    "\n",
    "        if uid in eval_method.train_set.uid_map and iid in eval_method.train_set.iid_map:\n",
    "            # convert uids to uidx\n",
    "            uid = eval_method.train_set.uid_map[uid]\n",
    "            # convert iids to iidx\n",
    "            iid = eval_method.train_set.iid_map[iid]\n",
    "            uid, iid = int(uid), int(iid)\n",
    "            ground_truth[uid].add(iid)\n",
    "\n",
    "        # uid = eval_method.test_set.uid_map[uid]\n",
    "        # iid = eval_method.test_set.iid_map[iid]\n",
    "\n",
    "        # uid, iid = int(uid), int(iid)\n",
    "        # ground_truth[uid].add(iid)\n",
    "    return ground_truth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5165b2c443f1095",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read train data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b17fe862e843bfed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_train_data(train_file):\n",
    "    \"\"\"\n",
    "    Read test set data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_file:\n",
    "      The train_file set data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    train_checkins:\n",
    "      A dictionary includes user with items in train data\n",
    "    pop: dictionary\n",
    "      A dictionary of all items alongside of its occurrences counter in the training data\n",
    "      example: {1198: 893, 1270: 876, 593: 876, 2762: 867}\n",
    "    \"\"\"\n",
    "    train_checkins = defaultdict(set)\n",
    "    pop_items = dict()\n",
    "    train_data = open(train_file, 'r').readlines()\n",
    "\n",
    "    for eachline in train_data:\n",
    "        uid, iid, _ = eachline.strip().split()\n",
    "\n",
    "        # convert uids to uidx\n",
    "        uid = eval_method.train_set.uid_map[uid]\n",
    "        # convert iids to iidx\n",
    "        iid = eval_method.train_set.iid_map[iid]\n",
    "\n",
    "        uid, iid = int(uid), int(iid)\n",
    "        # a dictionary of popularity of items\n",
    "        if iid in pop_items.keys():\n",
    "            pop_items[iid] += 1\n",
    "        else:\n",
    "            pop_items[iid] = 1\n",
    "        train_checkins[uid].add(iid)\n",
    "    return train_checkins, pop_items"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80016e1275f3d03f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1ff01dfac40f9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def catalog_coverage(predicted: list, catalog: list) -> float:\n",
    "    \"\"\"\n",
    "    Computes the catalog coverage for k lists of recommendations\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted : a list of lists\n",
    "        Ordered predictions\n",
    "        example: [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n",
    "    catalog: list\n",
    "        A list of all unique items in the training data\n",
    "        example: ['A', 'B', 'C', 'X', 'Y', Z]\n",
    "    k: integer\n",
    "        The number of observed recommendation lists\n",
    "        which randomly chose in our offline setup\n",
    "    Returns\n",
    "    ----------\n",
    "    catalog_coverage:\n",
    "        The catalog coverage of the recommendations as a percent rounded to 2 decimal places\n",
    "    ----------\n",
    "    Metric Definition:\n",
    "    Ge, M., Delgado-Battenfeld, C., & Jannach, D. (2010, September).\n",
    "    Beyond accuracy: evaluating recommender systems by coverage and serendipity.\n",
    "    In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260). ACM.\n",
    "    \"\"\"\n",
    "    predicted_flattened = [p for sublist in predicted for p in sublist]\n",
    "    L_predictions = len(set(predicted_flattened))\n",
    "    catalog_coverage = round(L_predictions / (len(catalog) * 1.0) * 100, 2)\n",
    "    return catalog_coverage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53198f4d7f3d67d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def novelty(predicted: list, pop: dict, u: int, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the novelty for a list of recommended items for a user\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted : a list of recommended items\n",
    "        Ordered predictions\n",
    "        example: ['X', 'Y', 'Z']\n",
    "    pop: dictionary\n",
    "        A dictionary of all items alongside of its occurrences counter in the training data\n",
    "        example: {1198: 893, 1270: 876, 593: 876, 2762: 867}\n",
    "    u: integer\n",
    "        The number of users in the training data\n",
    "    k: integer\n",
    "        The length of recommended lists per user\n",
    "    Returns\n",
    "    ----------\n",
    "    novelty:\n",
    "        The novelty of the recommendations in system level\n",
    "    mean_self_information:\n",
    "        The novelty of the recommendations in recommended top-N list level\n",
    "    ----------\n",
    "    Metric Definition:\n",
    "    Zhou, T., Kuscsik, Z., Liu, J. G., Medo, M., Wakeling, J. R., & Zhang, Y. C. (2010).\n",
    "    Solving the apparent diversity-accuracy dilemma of recommender systems.\n",
    "    Proceedings of the National Academy of Sciences, 107(10), 4511-4515.\n",
    "    \"\"\"\n",
    "    self_information = 0\n",
    "    for item in predicted:\n",
    "        if item in pop.keys():\n",
    "            item_popularity = pop[item] / u\n",
    "            item_novelty_value = np.sum(-np.log2(item_popularity))\n",
    "        else:\n",
    "            item_novelty_value = 0\n",
    "        self_information += item_novelty_value\n",
    "    novelty_score = self_information / k\n",
    "    return novelty_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81a3d5a62cdd7ef4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def precisionk(actual, predicted):\n",
    "    return 1.0 * len(set(actual) & set(predicted)) / len(predicted)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c65de9de086f35b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def recallk(actual, predicted):\n",
    "    return 1.0 * len(set(actual) & set(predicted)) / len(actual)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8babd86ec10db5c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ndcgk(actual, predicted):\n",
    "    idcg = 1.0\n",
    "    dcg = 1.0 if predicted[0] in actual else 0.0\n",
    "    for i, p in enumerate(predicted[1:]):\n",
    "        if p in actual:\n",
    "            dcg += 1.0 / np.log(i + 2)\n",
    "        idcg += 1.0 / np.log(i + 2)\n",
    "    return dcg / idcg"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2244c064830f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_ranking_matrices(model, total_users, total_items, topk):\n",
    "    S = np.zeros((total_users, total_items))\n",
    "    P = np.zeros((total_users, topk))\n",
    "\n",
    "    # for model in exp.models:\n",
    "    print(model.name)\n",
    "    for uid in tqdm(range(total_users)):\n",
    "        S[uid] = model.score(uid)\n",
    "        P[uid] = np.array(list(reversed(model.score(uid).argsort()))[:topk])\n",
    "\n",
    "    return S, P"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fd73b17cca2353",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahelp is a binary matrix in which an element of its is 1 if the corresponding element in P (which is an item index) is in ground truth.\n",
    "Actually is shows whether the rankied item in P is included in ground truth or not."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0f599fa27956a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_ground_truth_index(total_users, topk, P, train_checkins):\n",
    "    Ahelp = np.zeros((total_users, topk))\n",
    "    for uid in tqdm(range(total_users)):\n",
    "        for j in range(topk):\n",
    "            # convert user_ids to user_idx\n",
    "            # convert item_ids to item_idx\n",
    "            if P[uid][j] in train_checkins[uid]:\n",
    "                Ahelp[uid][j] = 1\n",
    "    return Ahelp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771df2121e064a9e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a set of IDs for each users group"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70d19fbf27f15be7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_item_groups(item_group_fpath: str, gid) -> set:\n",
    "    item_group = open(item_group_fpath, 'r').readlines()\n",
    "    item_ids = set()\n",
    "    for eachline in item_group:\n",
    "        iid = eachline.strip()\n",
    "        if iid in eval_method.train_set.iid_map:\n",
    "            # convert iids to iidx\n",
    "            iid = eval_method.train_set.iid_map[iid]\n",
    "            iid = int(iid)\n",
    "            item_ids.add(iid)\n",
    "            I[iid][gid] = 1\n",
    "    return item_ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c46003fa7f748e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_item_index(total_users, topk, no_item_groups):\n",
    "    Ihelp = np.zeros((total_users, topk, no_item_groups))\n",
    "    for uid in range(total_users):\n",
    "        for lid in range(topk):\n",
    "            # convert item_ids to item_idx\n",
    "            if P[uid][lid] in shorthead_item_ids:\n",
    "                Ihelp[uid][lid][0] = 1\n",
    "            elif P[uid][lid] in longtail_item_ids:\n",
    "                Ihelp[uid][lid][1] = 1\n",
    "    return Ihelp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f69ccae848536a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87cc00442a0eeb3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def metric_per_group(group, W):\n",
    "    NDCG10 = list()\n",
    "    Pre10 = list()\n",
    "    Rec10 = list()\n",
    "    Novelty10 = list()\n",
    "    predicted = list()\n",
    "    All_Predicted = list()\n",
    "\n",
    "    for uid in tqdm(group):\n",
    "        if uid in ground_truth.keys():\n",
    "            for j in range(50):\n",
    "                if W[uid][j].x == 1:\n",
    "                    predicted.append(P[uid][j])\n",
    "            copy_predicted = predicted[:]\n",
    "            All_Predicted.append(copy_predicted)\n",
    "            NDCG = ndcgk(actual=ground_truth[uid], predicted=predicted)\n",
    "            Pre = precisionk(actual=ground_truth[uid], predicted=predicted)\n",
    "            Rec = recallk(actual=ground_truth[uid], predicted=predicted)\n",
    "            Novelty = novelty(predicted=predicted, pop=pop_items, u=eval_method.total_users, k=10)\n",
    "\n",
    "            NDCG10.append(NDCG)\n",
    "            Pre10.append(Pre)\n",
    "            Rec10.append(Rec)\n",
    "            Novelty10.append(Novelty)\n",
    "\n",
    "            # cleaning the predicted list for a new user\n",
    "            predicted.clear()\n",
    "\n",
    "    catalog = catalog_coverage(predicted=All_Predicted, catalog=pop_items.keys())\n",
    "    return round(np.mean(NDCG10), 5), round(np.mean(Pre10), 5), round(np.mean(Rec10), 5), round(np.mean(Novelty10),\n",
    "                                                                                                5), catalog"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7586f659159f592",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def metric_on_all(W):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    predicted_user = list()\n",
    "    NDCG_all = list()\n",
    "    PRE_all = list()\n",
    "    REC_all = list()\n",
    "    Novelty_all = list()\n",
    "    All_Predicted = list()\n",
    "\n",
    "    for uid in tqdm(range(eval_method.total_users)):\n",
    "        if uid in ground_truth.keys():\n",
    "            for j in range(50):\n",
    "                if W[uid][j].x == 1:\n",
    "                    predicted_user.append(P[uid][j])\n",
    "\n",
    "            copy_predicted = predicted_user[:]\n",
    "            All_Predicted.append(copy_predicted)\n",
    "\n",
    "            NDCG_user = ndcgk(actual=ground_truth[uid], predicted=predicted_user)\n",
    "            PRE_user = precisionk(actual=ground_truth[uid], predicted=predicted_user)\n",
    "            REC_user = recallk(actual=ground_truth[uid], predicted=predicted_user)\n",
    "            Novelty_user = novelty(predicted=predicted_user, pop=pop_items, u=eval_method.total_users, k=10)\n",
    "\n",
    "            NDCG_all.append(NDCG_user)\n",
    "            PRE_all.append(PRE_user)\n",
    "            REC_all.append(REC_user)\n",
    "            Novelty_all.append(Novelty_user)\n",
    "\n",
    "            # cleaning the predicted list for a new user\n",
    "            predicted_user.clear()\n",
    "\n",
    "    catalog = catalog_coverage(predicted=All_Predicted, catalog=pop_items.keys())\n",
    "    return round(np.mean(NDCG_all), 5), round(np.mean(PRE_all), 5), round(np.mean(REC_all), 5), round(\n",
    "        np.mean(Novelty_all), 5), catalog"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b17607853f99b017",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fairness"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c59ace3198b5d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fairness_optimisation(fairness='N', uepsilon=0.000005, iepsilon=0.0000005):\n",
    "    print(f\"Runing fairness optimisation on '{fairness}', {format(uepsilon, 'f')}, {format(iepsilon, 'f')}\")\n",
    "    # V1: No. of users\n",
    "    # V2: No. of top items (topk)\n",
    "    # V3: No. of user groups\n",
    "    # V4: no. og item groups\n",
    "    V1, V2, V3, V4 = set(range(eval_method.total_users)), set(range(topk)), set(range(no_user_groups)), set(\n",
    "        range(no_item_groups))\n",
    "\n",
    "    # initiate model\n",
    "    model = Model()\n",
    "\n",
    "    # W is a matrix (size: user * top items) to be learned by model\n",
    "    # W = [[model.add_var(var_type=BINARY) for j in V2] for i in V1]\n",
    "    W = [[model.add_var() for j in V2] for i in V1]\n",
    "    user_dcg = [model.add_var() for i in V1]\n",
    "    user_ndcg = [model.add_var() for i in V1]\n",
    "    group_ndcg_v = [model.add_var() for k in V3]\n",
    "    item_group = [model.add_var() for k in V4]\n",
    "\n",
    "    user_precision = [model.add_var() for i in V1]\n",
    "    group_precision = [model.add_var() for k in V3]\n",
    "\n",
    "    user_recall = [model.add_var() for i in V1]\n",
    "    group_recall = [model.add_var() for k in V3]\n",
    "\n",
    "    if fairness == 'N':\n",
    "        ### No Fairness ###\n",
    "        model.objective = maximize(sum((S[i][j] * W[i][j]) for i in V1 for j in V2))\n",
    "    elif fairness == 'C':\n",
    "        ### C-Fairness: NDCG_Best: group_ndcg_v[1] - group_ndcg_v[0] ###\n",
    "        model.objective = maximize(\n",
    "            sum((S[i][j] * W[i][j]) for i in V1 for j in V2) - uepsilon * (group_ndcg_v[1] - group_ndcg_v[0]))\n",
    "    elif fairness == 'P':\n",
    "        model.objective = maximize(\n",
    "            sum((S[i][j] * W[i][j]) for i in V1 for j in V2) - iepsilon * (item_group[0] - item_group[1]))\n",
    "    elif fairness == 'CP':\n",
    "        model.objective = maximize(sum((S[i][j] * W[i][j]) for i in V1 for j in V2) - uepsilon * (\n",
    "                group_ndcg_v[1] - group_ndcg_v[0]) - iepsilon * (item_group[0] - item_group[1]))\n",
    "\n",
    "    # first constraint: the number of 1 in W should be equal to top-k, recommending top-k best items\n",
    "    k = 10\n",
    "    for i in V1:\n",
    "        model += xsum(W[i][j] for j in V2) == k\n",
    "\n",
    "    for i in V1:\n",
    "        user_idcg_i = 7.137938133620551\n",
    "\n",
    "        model += user_dcg[i] == xsum((W[i][j] * Ahelp[i][j]) for j in V2)\n",
    "        model += user_ndcg[i] == user_dcg[i] / user_idcg_i\n",
    "\n",
    "        model += user_precision[i] == xsum((W[i][j] * Ahelp[i][j]) for j in V2) / k\n",
    "        model += user_recall[i] == xsum((W[i][j] * Ahelp[i][j]) for j in V2) / len(train_checkins[i])\n",
    "\n",
    "    for k in V3:\n",
    "        model += group_ndcg_v[k] == xsum(user_dcg[i] * U[i][k] for i in V1)\n",
    "        model += group_precision[k] == xsum(user_precision[i] * U[i][k] for i in V1)\n",
    "        model += group_recall[k] == xsum(user_recall[i] * U[i][k] for i in V1)\n",
    "\n",
    "    for k in V4:\n",
    "        model += item_group[k] == xsum(W[i][j] * Ihelp[i][j][k] for i in V1 for j in V2)\n",
    "\n",
    "    for i in V1:\n",
    "        for j in V2:\n",
    "            model += W[i][j] <= 1\n",
    "    # optimizing\n",
    "    model.optimize()\n",
    "\n",
    "    return W, item_group"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d37146fa27c0956",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb52beb59167987"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def write_results():\n",
    "    ndcg_ac, pre_ac, rec_ac, novelty_ac, coverage_ac = metric_per_group(group=active_user_ids, W=W)\n",
    "    ndcg_iac, pre_iac, rec_iac, novelty_iac, coverage_iac = metric_per_group(group=inactive_user_ids, W=W)\n",
    "    ndcg_all, pre_all, rec_all, novelty_all, coverage_all = metric_on_all(W=W)\n",
    "    if fair_mode == 'N':\n",
    "        results.write(\n",
    "            f\"{dataset},{model.name},{u_group}%,{i_group}%,{fair_mode},-,-,{ndcg_all},{ndcg_ac},{ndcg_iac},{pre_all},{pre_ac},{pre_iac},{rec_all},{rec_ac},{rec_iac},{novelty_all},{novelty_ac},{novelty_iac},{coverage_all},{coverage_ac},{coverage_iac},{item_group[0].x},{item_group[1].x},{eval_method.total_users * 10}=={item_group[0].x + item_group[1].x}\")\n",
    "    elif fair_mode == 'C':\n",
    "        results.write(\n",
    "            f\"{dataset},{model.name},{u_group}%,{i_group}%,{fair_mode},{format(user_eps, '.7f')},-,{ndcg_all},{ndcg_ac},{ndcg_iac},{pre_all},{pre_ac},{pre_iac},{rec_all},{rec_ac},{rec_iac},{novelty_all},{novelty_ac},{novelty_iac},{coverage_all},{coverage_ac},{coverage_iac},{item_group[0].x},{item_group[1].x},{eval_method.total_users * 10}=={item_group[0].x + item_group[1].x}\")\n",
    "    elif fair_mode == 'P':\n",
    "        results.write(\n",
    "            f\"{dataset},{model.name},{u_group}%,{i_group}%,{fair_mode},-,{format(item_eps, '.7f')},{ndcg_all},{ndcg_ac},{ndcg_iac},{pre_all},{pre_ac},{pre_iac},{rec_all},{rec_ac},{rec_iac},{novelty_all},{novelty_ac},{novelty_iac},{coverage_all},{coverage_ac},{coverage_iac},{item_group[0].x},{item_group[1].x},{eval_method.total_users * 10}=={item_group[0].x + item_group[1].x}\")\n",
    "    elif fair_mode == 'CP':\n",
    "        results.write(\n",
    "            f\"{dataset},{model.name},{u_group}%,{i_group}%,{fair_mode},{format(user_eps, '.7f')},{format(item_eps, '.7f')},{ndcg_all},{ndcg_ac},{ndcg_iac},{pre_all},{pre_ac},{pre_iac},{rec_all},{rec_ac},{rec_iac},{novelty_all},{novelty_ac},{novelty_iac},{coverage_all},{coverage_ac},{coverage_iac},{item_group[0].x},{item_group[1].x},{eval_method.total_users * 10}=={item_group[0].x + item_group[1].x}\")\n",
    "    results.write('\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190318cf904a2356",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process\n",
    "\n",
    "The data is expected to be in plain text as a space separated UIR format ratings (test, train and val).\n",
    "Male users (active), Female users (inactive), Tracks by male artists (shorthead) and Tracks by female artists (longtail) are expected to be in plain text as a list of user ids and items ids."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b78a767301f44d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1: Iterate over the datasets\n",
    "for dataset in ds_names:\n",
    "    print(f\"Datasets: {dataset}\")\n",
    "\n",
    "    # Paths\n",
    "    root_path = './data/PyCPFair/'\n",
    "\n",
    "    # read train, tune, test datasets\n",
    "    train_data, tune_data, test_data = read_data(root_path=root_path)\n",
    "    # load data into Cornac and create eval_method\n",
    "    eval_method = load_data(train_data=train_data, test_data=test_data)\n",
    "    total_users = eval_method.total_users\n",
    "    total_items = eval_method.total_items\n",
    "    # load train_checkins and pop_items dictionary\n",
    "    train_checkins, pop_items = read_train_data(train_file=f\"{root_path}/train_ratings.txt\")\n",
    "    # load ground truth dict\n",
    "    ground_truth = read_ground_truth(test_file=f\"{root_path}/test_ratings.txt\")\n",
    "    # run Cornac models and create experiment object including models' results\n",
    "    exp = run_model(eval_method=eval_method)\n",
    "    # 4: read user groups\n",
    "    for u_group in ds_users:\n",
    "        # read matrix U for users and their groups\n",
    "        U = np.zeros((total_users, no_user_groups))\n",
    "\n",
    "        # Paths\n",
    "        active_users_path = f'{u_group}/male_users.txt'\n",
    "        inactive_users_path = f'{u_group}/female_users.txt'\n",
    "\n",
    "        # load active and inactive users\n",
    "        active_user_ids = read_user_groups(user_group_fpath=active_users_path, gid=0)\n",
    "        inactive_user_ids = read_user_groups(user_group_fpath=inactive_users_path, gid=1)\n",
    "        print(\n",
    "            f\"ActiveU: {len(active_user_ids)}, InActive: {len(inactive_user_ids)}, All: {len(active_user_ids) + len(inactive_user_ids)}\")\n",
    "        len_sizes = [len(active_user_ids), len(inactive_user_ids)]\n",
    "        # 5: read item groups\n",
    "        for i_group in ds_items:\n",
    "            # read matrix I for items and their groups\n",
    "            I = np.zeros((total_items, no_item_groups))\n",
    "\n",
    "            # Paths\n",
    "            shorthead_item_path = f'{i_group}/tracks_by_male_artists.txt'\n",
    "            longtail_item_path = f'{i_group}/tracks_by_female_artists.txt'\n",
    "\n",
    "            # read item groups\n",
    "            shorthead_item_ids = read_item_groups(item_group_fpath=shorthead_item_path, gid=0)\n",
    "            longtail_item_ids = read_item_groups(item_group_fpath=longtail_item_path, gid=1)\n",
    "            print(\n",
    "                f\"No. of Shorthead Items: {len(shorthead_item_ids)} and No. of Longtaill Items: {len(longtail_item_ids)}\")\n",
    "            # 2: iterate over the models\n",
    "            for model in exp.models:\n",
    "                results = open(f\"./results/PyCPFair/results_{dataset}_{model.name}.csv\", 'w')\n",
    "                results.write(\n",
    "                    \"Dataset,Model,GUser,GItem,Type,User_EPS,Item_EPS,ndcg_ALL,ndcg_ACT,ndcg_INACT,Pre_ALL,Pre_ACT,Pre_INACT,Rec_ALL,Rec_ACT,Rec_INACT,Nov_ALL,Nov_ACT,Nov_INACT,Cov_ALL,Cov_ACT,Cov_INACT,Short_Items,Long_Items,All_Items\\n\")\n",
    "                print(f\"> Model: {model.name}\")\n",
    "                # load matrix S and P\n",
    "                S, P = load_ranking_matrices(model=model, total_users=total_users, total_items=total_items, topk=topk)\n",
    "                # load matrix Ahelp\n",
    "                Ahelp = load_ground_truth_index(total_users=total_users, topk=topk, P=P, train_checkins=train_checkins)\n",
    "                # load matrix Ihelp\n",
    "                Ihelp = read_item_index(total_users=total_users, topk=50, no_item_groups=no_item_groups)\n",
    "                # iterate on fairness mode: user, item, user-item\n",
    "                for fair_mode in ['N', 'CP']:\n",
    "                    if fair_mode == 'N':\n",
    "                        W, item_group = fairness_optimisation(fairness=fair_mode)\n",
    "                        write_results()\n",
    "                    elif fair_mode == 'C':\n",
    "                        for user_eps in [0.003, 0.0005, 0.0001, 0.00005, 0.000005]:\n",
    "                            W, item_group = fairness_optimisation(fairness=fair_mode, uepsilon=user_eps)\n",
    "                            write_results()\n",
    "                    elif fair_mode == 'P':\n",
    "                        for item_eps in [0.003, 0.0005, 0.0001, 0.00005, 0.000005]:\n",
    "                            W, item_group = fairness_optimisation(fairness=fair_mode, iepsilon=item_eps)\n",
    "                            write_results()\n",
    "                    elif fair_mode == 'CP':\n",
    "                        for user_eps in [0.003]:\n",
    "                            for item_eps in [0.003]:\n",
    "                                W, item_group = fairness_optimisation(fairness=fair_mode, uepsilon=user_eps,\n",
    "                                                                      iepsilon=item_eps)\n",
    "                                write_results()\n",
    "                results.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29a0416ae27138cb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
