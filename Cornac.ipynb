{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "These experiments were run on Python 3.8. In the requirements.txt are the versions used for these packages.\n",
    "- tqdm: For showing progress in loops.\n",
    "- numpy and pandas: For data manipulation.\n",
    "- cornac: For obtaining the recommendations.\n",
    "- tensorflow: Required by cornac.\n",
    "- torch: Required for the VAECF implementation of Cornac."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69ba81caf58e1381"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from logging import Formatter, StreamHandler, getLogger, INFO\n",
    "\n",
    "from tqdm import tqdm\n",
    "from cornac import Experiment\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.metrics import NDCG, Recall, Precision\n",
    "from cornac.models import MF, WMF, SVD, VAECF\n",
    "from cornac.exception import ScoreException\n",
    "from numpy import array, nan\n",
    "from pandas import read_csv, DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logger setup\n",
    "Here we set up the logger for showing some info when executing this script."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "419102f5de0b9dd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "ch = StreamHandler()\n",
    "ch.setLevel(INFO)\n",
    "ch.setFormatter(\n",
    "    Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    ")\n",
    "\n",
    "logger.addHandler(ch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2daab4dcf47f0e24",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration variables\n",
    "We define some variables used on the rest of the experiment.\n",
    "\n",
    "### General config\n",
    "Getting the date now and the name of the experiment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69c488da52c0b011"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "now = f'{datetime.now():%Y%m%d%H%M%S}'\n",
    "experiment_name = 'AMBAR'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d635b3d3d5b23e3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### File and dir config\n",
    "Getting the working directory with pathlib, and obtaining the csv to be used in cornac, and defining a results directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d68c6a66141bf440"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "work_dir = Path('.').resolve()\n",
    "data_file = work_dir / 'data' / experiment_name / 'ratings_info.csv'\n",
    "results_dir = work_dir / 'results' / 'AMBAR' / experiment_name / now"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b5c46e25bce6b97",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we make sure the results directory exists by creating it if it doesn't."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51611283d1977550"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a635f7c30ba080d2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we make sure the data file exists and is a file. Here we could also make sure that the file is an actual csv file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaad9a156a91656b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not data_file.exists() and data_file.is_file():\n",
    "    print(\"Bad data file\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97ea640067e44fd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataframe config\n",
    "We define the names of the headers of each column to be identified by pandas. Also, we define the data type of the values in each cell of the user, item and rating. If the data has multiple data types, the val_dtype can be a list of type string compatible with pandas."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b5bb98578bc357"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "col_names = {\n",
    "    'user': 'user_id',\n",
    "    'item': 'track_id',\n",
    "    'rating': 'rating'\n",
    "}\n",
    "val_dtype = 'int'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79e39b6dbc03ed72",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cornac config\n",
    "Here we set up the k value, the test set size and the validation set size. Also we decide if we want to exclude unknown values or not."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "144f3e896f286446"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "k = 1000\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "exclude_unknown = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f569e7c7b4df6096",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function setup\n",
    "We set up various utility functions to be used later. Mostly for exporting data and getting it in a format compatible with cornac.\n",
    "\n",
    "set_data_to_tuple_list takes a dict of {user: [item_list, rating_list]}, process it and returns a tuple list of format [(user, item, rating)...]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bab5851fd6f35830"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def set_data_to_tuple_list(d: dict) -> list:\n",
    "    result = []\n",
    "    for user in d:\n",
    "        transpose = array(d[user]).T\n",
    "        for t in transpose:\n",
    "            result.append((user,) + tuple(t))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f809e7ca6f8cedc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "list_to_dict converts a list into a dict using dict comprehension and enumerate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb5bd6bbdad0dba2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def list_to_dict(l: list) -> dict:\n",
    "    return {i: v for i, v in enumerate(l)}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "692ef4f13a3a5c51",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "get_set_dataframe process the raw data ({user: [item_list, rating_list]}), with the item ids and user ids, and converts it into a pandas DataFrame to be exported later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b75cb4025f5f7fa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_set_dataframe(set_data: dict, i_ids: list, u_ids: list) -> DataFrame:\n",
    "    data_list = set_data_to_tuple_list(set_data)\n",
    "    i_map = list_to_dict(i_ids)\n",
    "    u_map = list_to_dict(u_ids)\n",
    "\n",
    "    set_df = DataFrame(data_list,\n",
    "                       columns=list(col_names.values()),\n",
    "                       dtype=val_dtype)\n",
    "    set_df['item_idx'] = set_df[col_names['item']]\n",
    "    set_df['item'] = set_df[col_names['item']].replace(to_replace=i_map)\n",
    "    set_df['user_idx'] = set_df[col_names['user']]\n",
    "    set_df['user'] = set_df[col_names['user']].replace(to_replace=u_map)\n",
    "    return set_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0f5aba5f16befe8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Experiment start...')\n",
    "logger.info(f'{experiment_name}')\n",
    "logger.info(f'{k=}')\n",
    "logger.info(f'{work_dir=}')\n",
    "logger.info(f'{data_file=}')\n",
    "logger.info(f'{results_dir=}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f3457a3965bf91",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create the dataset out of the data file, the expected data is only with user, item and rating in that order. The name of the columns is defined in the set-up part, same with the data types.\n",
    "\n",
    "For testing purposes before actually executing the full experiment, we left a filter that takes a sample of 50 users, and gets only the data of those 50 users. Please use it only to make sure that the script executes correctly from start to finish."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7017668614f3b926"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "keys = ['0', '1', '2']\n",
    "\n",
    "if isinstance(val_dtype, str):\n",
    "    d_type = {key: val_dtype for key in keys}\n",
    "elif isinstance(val_dtype, list):\n",
    "    d_type = dict(zip(keys, val_dtype))\n",
    "else:\n",
    "    logger.error('Wrong type setup. Must be a type string or a list of type string.')\n",
    "    exit()\n",
    "\n",
    "logger.info('Loading data into triplets...')\n",
    "df = read_csv(\n",
    "    data_file,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "# user_filter = Series(df['0'].unique()).sample(50).to_list()\n",
    "# df = df[df['0'].isin(user_filter)]\n",
    "\n",
    "data = list(df.to_records(index=False, column_dtypes=d_type))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d16d3a7ebfcee39b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create the Ratio Split that will be used by cornac. It splits the data into 3 sets randomly. 1 for test, 1 for train and 1 for validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "300bbb3260fd4305"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Creating ratio split...')\n",
    "ratio_split = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=test_size,\n",
    "    val_size=val_size,\n",
    "    exclude_unknowns=exclude_unknown,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e6e383e56136f1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the metris here. In this experiment, we set up NDCG, Recall and Precision, using the k defined in the set-up."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ab41cdeeccb9adf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    NDCG(k),\n",
    "    Recall(k),\n",
    "    Precision(k)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a2aa4217a63dfc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we define the models with some previously obtained parameters. We could also define the hyperparameter calculation in this part, in this case, is important to leave a models variable with said configuration, so cornac can pick up the array and execute the calculation and exporting of the recommendations.\n",
    "\n",
    "Because this script is assuming an array with models with parameters already predefined, in case of needing the best parameters obtained by cornac, the exporting of this must be done after running the experiment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a52e4a74ce9da083"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "models = [\n",
    "    MF(\n",
    "        max_iter=5,\n",
    "        k=k,\n",
    "        early_stop=True,\n",
    "        verbose=True,\n",
    "        lambda_reg=0.001,\n",
    "        learning_rate=0.01,\n",
    "        use_bias=False\n",
    "    ),\n",
    "    WMF(\n",
    "        k=k,\n",
    "        max_iter=50,\n",
    "        learning_rate=0.001,\n",
    "        lambda_u=0.01,\n",
    "        lambda_v=0.01,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    SVD(\n",
    "        max_iter=5,\n",
    "        k=k,\n",
    "        early_stop=True,\n",
    "        verbose=True,\n",
    "        lambda_reg=0.0001,\n",
    "        learning_rate=0.0001\n",
    "    ),\n",
    "    VAECF(\n",
    "        k=k,\n",
    "        autoencoder_structure=[20],\n",
    "        act_fn=\"tanh\",\n",
    "        likelihood=\"mult\",\n",
    "        n_epochs=100,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "        beta=1.0,\n",
    "        use_gpu=True,\n",
    "        verbose=True\n",
    "    ),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d689a754ab1d8510",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_users = ratio_split.train_set.num_users\n",
    "total_items = ratio_split.train_set.num_items\n",
    "logger.info(f'{total_users=}')\n",
    "logger.info(f'{total_items=}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9d059a8baf22a2d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the metrics and models, we export the test, train and validation data into the results directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78d75765eba73247"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Exporting test data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.test_set.user_data),\n",
    "    list(ratio_split.test_set.item_ids),\n",
    "    list(ratio_split.test_set.user_ids),\n",
    ").to_csv(results_dir / 'test_set.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3120168e041a6a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Exporting train data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.train_set.user_data),\n",
    "    list(ratio_split.train_set.item_ids),\n",
    "    list(ratio_split.train_set.user_ids),\n",
    ").to_csv(results_dir / 'train_set.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd0d1fd06dba550",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Exporting validation data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.val_set.user_data),\n",
    "    list(ratio_split.val_set.item_ids),\n",
    "    list(ratio_split.val_set.user_ids),\n",
    ").to_csv(results_dir / 'val_set.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6986140f5a050d1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we run the experiments with the defined variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19dc57886ed53253"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Running experiment...')\n",
    "exp = Experiment(\n",
    "    eval_method=ratio_split,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    ")\n",
    "exp.run()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eee47a93bb6b755c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running the experiment, we export the metrics obtained from the calculation into a csv using pandas."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3faff40fa70f6cf3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Exporting metrics...')\n",
    "metric_results = {\n",
    "    exp.models[i].name: dict(exp.result[i].metric_avg_results)\n",
    "    for i in range(len(models))\n",
    "}\n",
    "(DataFrame(metric_results)\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'metric'})\n",
    " .to_csv(results_dir / 'metric_results.csv'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b07b42b349bb38a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally we export the recommendations. We use a custom multi loop to get the results.\n",
    "- Here we first loop over the models of the experiment.\n",
    "- We loop over the users map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the scores for the users.\n",
    "- We get the k top items using a combination of argsort and reversing of the list.\n",
    "- We loop over the items map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the score obtained from cornac, or nan in case of IndexError.\n",
    "- We append the user and items, both the id and indexes, and the score to the result list.\n",
    "- After all the loops are finished, we export the data into a csv file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4ad9fa00e924ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info('Processing models...')\n",
    "for model in exp.models:\n",
    "    model_result = []\n",
    "    logger.info(f'Getting scores for {model.name}...')\n",
    "\n",
    "    for user_id, user_index in tqdm(exp.eval_method.train_set.uid_map.items()):\n",
    "        try:\n",
    "            scores = model.score(user_index)\n",
    "        except ScoreException:\n",
    "            logger.error(f\"{model.name}: Couldn't predict for user {user_index} ({user_id=})\")\n",
    "            continue\n",
    "\n",
    "        top_items = list(reversed(scores.argsort()))[:k]\n",
    "\n",
    "        for item_id, item_index in exp.eval_method.train_set.iid_map.items():\n",
    "            if item_index not in top_items:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                score = scores[item_index]\n",
    "            except IndexError:\n",
    "                logger.error(\n",
    "                    f\"{model.name}: No score for item {item_index} ({item_id=}) in user {user_index} ({user_id=})\"\n",
    "                )\n",
    "                score = nan\n",
    "\n",
    "            model_result.append({\n",
    "                'user_id': user_id,\n",
    "                'user_index': user_index,\n",
    "                'item_id': item_id,\n",
    "                'item_index': item_index,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "    logger.info(f'Exporting {model.name}...')\n",
    "    (DataFrame(model_result)\n",
    "     .sort_values(by=['user_id', 'score'], ascending=[True, False])\n",
    "     .to_csv(results_dir / f'{model.name}.csv'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa3b156d0a731c3e",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
